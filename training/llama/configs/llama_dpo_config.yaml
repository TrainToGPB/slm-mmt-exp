training:
  plm_name: beomi/Llama-3-Open-Ko-8B
  use_fsdp: False
  torch_dtype: bfloat16
  output_dir: models/mmt-ft/ko-enjazh/v1-it-dpo-xml
  dataloader_num_workers: 4
  per_device_batch_size: 8
  max_src_length: 4000
  max_new_length: 4000
  max_length: 8192
  num_epochs: 1
  learning_rate: 1e-4
  warmup_ratio: 0.10
  lr_scheduler_type: constant_with_warmup
  optim: paged_adamw_32bit
  gradient_checkpointing: True
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0
  weight_decay: 0.01
  bf16: True # at least one of fp16 / bf16 should be False
  logging_dir: ./logs
  logging_strategy: steps
  evaluation_strategy: steps
  save_strategy: steps
  logging_steps: 25
  eval_steps: 100 # warmup_steps(total_steps의 10%)로 대체
  save_steps: 100 # warmup_steps(total_steps의 10%)로 대체
  save_total_limit: 2
  seed: 42
  report_to: wandb
  eval_accumulation_steps: 1
  eos_token_id: 128001
  pad_token_id: 128002
  load_best_model_at_end: True
  metric_for_best_model: loss
  remove_unused_columns: True

data: 
  train_dataset_name: /data/sehyeong/nmt/datasets/mmt-dpo/

peft:
  use_4bit: False
  bnb_4bit_quant_type: nf4
  bnb_4bit_compute_dtype: bfloat16
  use_double_quant: True
  use_lora: True
  adapter_name: mmt-v1

dpo:
  dpo_loss_type: sigmoid # sigmoid, hinge(RSO), ipo, kto_pair, bco_pair, sppo_hard, nca_pair, robust
  dpo_beta: 0.1
  pol_adapter_name: pol-v1
  ref_adapter_name: ref-v1

general:
  project_name: mmt-v1
  run_name: v1-dpo-it-xml
