training:
  seed: 42
  output_dir: ../models
  plm_name: facebook/mbart-large-50
  src_lang: en_XX
  tgt_lang: ko_KR
  use_fsdp: True
  dataloader_num_workers: 4
  per_device_batch_size: 4
  save_total_limit: 2
  num_train_epochs: 5
  learning_rate: 3e-5
  lr_scheduler_type: linear
  gradient_accumulation_steps: 4
  warmup_ratio: 0.1
  weight_decay: 0.01
  fp16: True
  logging_dir: ./logs
  logging_strategy: steps
  evaluation_strategy: steps
  save_strategy: steps
  logging_steps: 100
  eval_steps: 1000
  save_steps: 1000
  report_to: wandb
  load_best_model_at_end: True
  metric_for_best_model: SacreBLEU

data:
  dataset_name: traintogpb/aihub-koen-translation-integrated-tiny-100k

general:
  project_name: ojt_translation # ojt_translation
  run_name: mbart-baseline-2
  